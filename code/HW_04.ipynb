{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Multi-Layer-Perceptron\n",
    "<img src=\"assets/Hull-Robert-HW-04-0f0ceaa4.png\" width=\"600\" />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "See https://colab.research.google.com/drive/1G4oFN-__Zlh7Fw-hTa0S1gHkXtuUUJy-?usp=sharing\n",
    "\n",
    "* Resources used to make this assignment:\n",
    "  * The 'load CIFAR' dataset heavilty borrowed from pytorch tutorial - https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "  * A refresher on the MLP borrowed from machine learning mastery - https://machinelearningmastery.com/neural-networks-crash-course/\n",
    "  * Some code borrowed from medium . com about putting MLP into a pytorch - https://medium.com/@aungkyawmyint_26195/multi-layer-perceptron-mnist-pytorch-463f795b897a\n",
    "  * Some notes on regularization in MLP:\n",
    "    * https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.5-Regularization.pdf\n",
    "  * Very helpful for avoiding a blocker with batch_size dimensions. Be very careful with linear layers as the first parameter is batch_size, this is different than for convolutional layers\n",
    "    * https://discuss.pytorch.org/t/valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498valueerror-expected-input-batch-size-324-to-match-target-batch-size-4/24498\n",
    "    * https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd\n",
    "  * L2 normalization is implemented in the optimizer\n",
    "    * https://pytorch.org/docs/stable/optim.html\n",
    "  * How to save a model object\n",
    "    * https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "* Question:\n",
    "    * How to get CIFAR10 datasets? \n",
    "        * https://piazza.com/class/kipjbnzn39r4q7?cid=66\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Adaboost\n",
    "<img src=\"assets/Hull-Robert-HW-04-af3bc12d.png\" width=\"600\" />\n",
    "\n",
    "<img src=\"assets/01_HW4-2.png\" width=\"600\" />\n",
    "\n",
    "<img src=\"assets/02_AdaBoost_PC_02.png\" width=\"600\" />\n",
    "\n",
    "<img src=\"assets/Screen Shot 2021-03-28 at 1.45.10 PM.png\" width=\"600\" />\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* sampling distribution from HW 1\n",
    "* sklearn algorithm for Adaboost\n",
    "* weak learning algorithm = shallow decision tree\n",
    "\n",
    "Background\n",
    "\n",
    "    * Understanding AdaBoost - https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe\n",
    "    * SKLearn - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "    * Machine Learning Mastery \n",
    "    * SKLearn - https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. Recurrent Neural Networks for Language Modeling\n",
    "<img src=\"assets/Hull-Robert-HW-04-881d387a.png\" width=\"600\" />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Links:\n",
    "    - To original dataset: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "    \n",
    "\n",
    "LSTM: A search space odyssey\n",
    "- lots of varieties of LSTM\n",
    "- But Vanilla is the best\n",
    "- The forget gate and output activation function are its most critical components\n",
    "- guidelines for efficient hyperparameter adjustment\n",
    "\n",
    "    Intro:\n",
    "    - The central idea behind the LSTM architecture is a memory cell which can maintain its state over time, and non-linear gating units which regulate the information flow into and out of the cell. \n",
    "    - random search is used to find best-performing hyperparameters\n",
    "\n",
    "    Vanilla LSTM:\n",
    "    - "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}