{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hull_HW_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIuXN19CLZXA"
      },
      "source": [
        "# HW_04 Problem #3\n",
        "\n",
        "<img src=\"assets/Hull-Robert-HW-04-881d387a.png\" width=\"600\" />\n",
        "\n",
        "### This Code was written by Robert 'Quinn' Hull, and borrowed elements from several other resources:\n",
        "\n",
        "* The bulk of this script is the TensorFlow article about text generation: https://www.tensorflow.org/tutorials/text/text_generation\n",
        "* Much of the text was removed to make this shorter and more intelligible\n",
        "* The text describing my work is available at the end of the script. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GCCk8_dHpuNf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the Shakespeare dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_55cOxLkAb"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b89fb660-6525-41c8-d232-54beb18f57f9"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# # subset training text (temporary)\n",
        "# text = text[:11153]\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n",
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GMlCe3qzaL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a4d44c-59af-4088-decb-39808b55b4a8"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "print('characters in ', chars)\n",
        "\n",
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "ids\n",
        "\n",
        "print('IDs ', ids)\n",
        "\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "chars = chars_from_ids(ids)\n",
        "chars\n",
        "\n",
        "print('characters out ', chars)\n",
        "\n",
        "# You can `tf.strings.reduce_join` to join the characters back into strings. \n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "characters in  <tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n",
            "IDs  <tf.RaggedTensor [[41, 42, 43, 44, 45, 46, 47], [64, 65, 66]]>\n",
            "characters out  <tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples, targets, batches\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4bc4a9-f537-405f-d472-e53f7c392af5"
      },
      "source": [
        "# Create examples and targets\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "# set constants\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# set batches\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# split dataset\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# create training batches\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "source": [
        "# models\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else: \n",
        "      return x\n",
        "\n",
        "# new models, an LSTM\n",
        "class NewModel1(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=False) # change from GRU to LSTM\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    # print(states)\n",
        "    # print(training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    # print(states)\n",
        "    x = self.lstm(x) # , initial_state=states) # , training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else: \n",
        "      return x\n",
        "\n",
        "# new models2, an LSTM\n",
        "class NewModel2(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True) # change from GRU to LSTM\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    # print(states)\n",
        "    # print(training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    # print(states)\n",
        "    x = self.lstm(x , initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else: \n",
        "      return x"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "source": [
        "# model chars\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab) # QH NOTE: this = 65. \n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = int(1024) # 1024\n",
        "\n",
        "# set model\n",
        "model = NewModel2(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)\n",
        "\n",
        "# optimizer and loss function\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# set number of epochs\n",
        "EPOCHS = 10"
      ],
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0l5U1zoYtNc"
      },
      "source": [
        "### Easy Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "outputId": "a7763a2c-0f90-4386-bd3d-e1d2b0a40286"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "# train model (naive)\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-287-28b3bf9f2177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# train model (naive)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-279-ea55be33f8d6>:69 call  *\n        x = self.dense(x, training=training)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__  **\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer dense_28 expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'new_model2_1/lstm_15/PartitionedCall:1' shape=(64, 100, 1024) dtype=float32>, <tf.Tensor 'new_model2_1/lstm_15/PartitionedCall:2' shape=(64, 1024) dtype=float32>, <tf.Tensor 'new_model2_1/lstm_15/PartitionedCall:3' shape=(64, 1024) dtype=float32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "    \n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "source": [
        "temp = 1.0\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature=temp)"
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b8c461-3da5-4fd2-f696-504d5a3ff57a"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "pred_char_len = 50\n",
        "\n",
        "for n in range(pred_char_len):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "\n",
        "print(f\"\\nRun time: {end - start}\")"
      ],
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Ag:\n",
            "Ange notifaloulyo ced bleathird thostll?\n",
            "Thit \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 0.8481040000915527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuxjNkhMkiVb"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjFdVka0klET"
      },
      "source": [
        "### Bleu\n",
        "* From: https://towardsdatascience.com/how-to-evaluate-text-generation-models-metrics-for-automatic-evaluation-of-nlp-models-e1c251b04ec1\n",
        "* BLEU is a precision focused metric that calculates n-gram overlap of the reference and generated texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnWYvrvhkkKU"
      },
      "source": [
        "# prep\n",
        "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n",
        "\n",
        "def bleu(ref, gen):\n",
        "    ''' \n",
        "    calculate pair wise bleu score. uses nltk implementation\n",
        "    Args:\n",
        "        For word comparison\n",
        "          references : a list of reference sentences \n",
        "          candidates : a list of candidate(generated) sentences\n",
        "        For character comparison\n",
        "          references : a list of reference sentences \n",
        "          candidates : a list of candidate(generated) sentences\n",
        "    Returns:\n",
        "        bleu score(float)\n",
        "    '''\n",
        "    ref_bleu = []\n",
        "    gen_bleu = []\n",
        "    print('test1')\n",
        "    for l in gen:\n",
        "        print('test2')\n",
        "        gen_bleu.append(l.split())\n",
        "    print(gen_bleu)\n",
        "    for i,l in enumerate(ref):\n",
        "        print('test3')\n",
        "        ref_bleu.append([l.split()])\n",
        "    print(ref_bleu)\n",
        "    cc = SmoothingFunction()\n",
        "    print(cc)\n",
        "    score_bleu = corpus_bleu(ref_bleu, gen_bleu) \n",
        "    return score_bleu"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKQfoB56lGlv"
      },
      "source": [
        "bleu_score = 'NA' # bleu(['hillo'],['hello'])"
      ],
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn3rurTrlbJL"
      },
      "source": [
        "# Final Comments and Text\n",
        "* In general, the script experiments with:\n",
        "  * hyperparameters: \n",
        "     * embedding_dim, rnn_units, temperature, epochs\n",
        "  * Model Structure\n",
        "    * LSTM v RNN\n",
        "* Lingering questions I have:\n",
        "  * I was mostly unsuccessful at using an LSTM for this. It's definitely in how I am setting up this model.\n",
        "  * I never figured out how to properly assess this model. I started to experiment with the BLEU algorithm, but I wasn't sure this is built for character generation in this context. I.E., I wasn't sure how to apply this to our use-case where the text generated was mostly 'random'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIk7NiuTla0j",
        "outputId": "a953142c-516e-46e8-9451-315f5f48f96d"
      },
      "source": [
        "cats = ['bleu_score', 'Training loss (final)', 'Training sequence length', \n",
        "        'Training Buffer Size', 'Training Epochs', 'Training Batch Size', \n",
        "        'RNN model: RNN units', 'RNN model: Embedding Dim', \n",
        "        'RNN model: Name of loss function', 'RNN model: Summary', 'RNN model: history object',\n",
        "        'Prediction: character length', 'Prediction: temp constant', 'Prediction: result',\n",
        "        'Prediction: model object']\n",
        "      \n",
        "print(cats, '\\n')\n",
        "\n",
        "# save1 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "#          BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "#          rnn_units, embedding_dim,\n",
        "#          loss.name, model, history,\n",
        "#          pred_char_len, temp, result, \n",
        "#          one_step_model]\n",
        "print('Model 1 is the `custom` model using the customized model object in the original code\\n',\n",
        "      'There is no calculation of bleu score for this one, because I havent figured out at this point\\n',\n",
        "      'how to best evaluate the model. \\n')\n",
        "print(save1, '\\n\\n')\n",
        "\n",
        "# save2 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "#          BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "#          rnn_units, embedding_dim,\n",
        "#          loss.name, model, history,\n",
        "#          pred_char_len, temp, result, \n",
        "#          one_step_model]\n",
        "print('Model 2 is our baseline model using the class MyModel with architecture the same as in the original code\\n',\n",
        "      'From the persective of training loss, it performs nearly as well as the previous custom model\\n',\n",
        "      np.round(save2[1],2), 'versus', np.round(save1[1],2), '\\n')\n",
        "print(save2, '\\n\\n')\n",
        "\n",
        "# save3 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "#          BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "#          rnn_units, embedding_dim,\n",
        "#          loss.name, model, history,\n",
        "#          pred_char_len, temp, result, \n",
        "#          one_step_model]\n",
        "print('Model 3 is our baseline model using the class MyModel with architecture the same as in the original code\\n',\n",
        "      'to highlight the impact of epochs on training performance, we have reduced it from',save2[4], 'to', save3[4], '\\n',\n",
        "      'From the persective of training loss, it performs worse than before',np.round(save3[1],2), 'versus', np.round(save2[1],2), '\\n',\n",
        "      'The predicted text is less coherent, too:\\n', 'Model 3 :', save3[13].numpy()[0], '\\n', 'Model 2 :', save2[13].numpy()[0], '\\n')\n",
        "print(save3, '\\n\\n')\n",
        "\n",
        "# save4 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "#          BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "#          rnn_units, embedding_dim,\n",
        "#          loss.name, model, history,\n",
        "#          pred_char_len, temp, result, \n",
        "#          one_step_model]\n",
        "print('Model 4 is our baseline model using the class MyModel with architecture the same as in the original code\\n',\n",
        "      'to highlight the impact of epochs on training performance, we have increased it from',save3[4], 'to', save4[4], '\\n',\n",
        "      'From the persective of training loss, it performs better than before',np.round(save4[1],2), 'versus', np.round(save3[1],2), '\\n',\n",
        "      'The predicted text is way more coherent, almost Shakespearean:\\n', 'Model 4 :', save4[13].numpy()[0], '\\n', 'Model 3 :', save3[13].numpy()[0], '\\n',\n",
        "      'This performance bump does come at the expense of time, though (20s / epoch * 30 epochs = 10 minutes, \\n')\n",
        "print(save4, '\\n\\n')\n",
        "\n",
        "# save5 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "#          BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "#          rnn_units, embedding_dim,\n",
        "#          loss.name, model, history,\n",
        "#          pred_char_len, temp, result, \n",
        "#          one_step_model]\n",
        "print('Model 5 is our baseline model using the class MyModel with architecture the same as in the original code\\n',\n",
        "      'to explore ways to speed up training and preserve text coherence, I reduce the size of the input text \\n',\n",
        "      'by two orders of magnitude: 1115394 characters to 11153 characters and keep epochs =',save5[4], '\\n',\n",
        "      'From the persective of training loss, it performs far worse than before',np.round(save5[1],2), 'versus', np.round(save4[1],2), '\\n',\n",
        "      'The predicted text is nonsense :\\n', 'Model 6 :', save5[13].numpy()[0], '\\n', 'Model 5 :', save4[13].numpy()[0], '\\n',\n",
        "      'This might be because of undertraining, or an issue in how weve indexed the vocabulary \\n',\n",
        "      'This saves training speed substantially! - 1 s / epoch \\n')\n",
        "print(save5, '\\n\\n')\n",
        "\n",
        "# save6 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "#          BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "#          rnn_units, embedding_dim,\n",
        "#          loss.name, model, history,\n",
        "#          pred_char_len, temp, result, \n",
        "#          one_step_model]\n",
        "print('Model 6 is our baseline model using the class MyModel, returning to epochs = ', save2[4], 'and characters = 1115394\\n',\n",
        "      'Here we start to vary the architecture of the model, so rnn_units =', save6[6], 'up from', save2[6], '\\n',\n",
        "      'From the persective of training loss, it performs better than baseline (Model 2)',np.round(save6[1],2), 'versus', np.round(save2[1],2), '\\n',\n",
        "      'The predicted text is probably more coherent :\\n', 'Model 6 :', save6[13].numpy()[0], '\\n', 'Model 2 :', save2[13].numpy()[0], '\\n',\n",
        "      'Worth noting that the run-time nearly tripled, from 20 s / epoch to 55 s / epoch \\n')\n",
        "print(save6, '\\n\\n')\n",
        "\n",
        "# save7 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "#          BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "#          rnn_units, embedding_dim,\n",
        "#          loss.name, model, history,\n",
        "#          pred_char_len, temp, result, \n",
        "#          one_step_model]\n",
        "print('Model 7 is our baseline model using the class MyModel \\n',\n",
        "      'Where we decrease the rnn_units =', save7[6], 'down from', save6[6], '\\n',\n",
        "      'From the persective of training loss, it performs worse than baseline (Model 2)',np.round(save7[1],2), 'versus', np.round(save2[1],2), '\\n',\n",
        "      'The predicted text is arguably no more or less coherent :\\n', 'Model 7 :', save7[13].numpy()[0], '\\n', 'Model 2 :', save2[13].numpy()[0], '\\n',\n",
        "      'Worth noting that the run-time didnt change very much \\n ')\n",
        "print(save7, '\\n\\n')\n",
        "\n",
        "# save8 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "#          BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "#          rnn_units, embedding_dim,\n",
        "#          loss.name, model, history,\n",
        "#          pred_char_len, temp, result, \n",
        "#          one_step_model]\n",
        "print('Model 8 is our first LSTM model using the class NewModel1 \\n',\n",
        "      'Where we return the rnn_units =', save8[6], '\\n',\n",
        "      'From the persective of training loss, it performs _____ than baseline (Model 2)',np.round(save8[1],2), 'versus', np.round(save2[1],2), '\\n',\n",
        "      'The predicted text is totally incoherent :\\n', 'Model 8 :', save8[13].numpy()[0], '\\n',\n",
        "      'I have definitely made an error in how I set up this LSTM network \\n ')\n",
        "print(save8, '\\n\\n')\n",
        "\n",
        "save9 = [bleu_score, history.history['loss'][-1], seq_length, \n",
        "         BUFFER_SIZE, EPOCHS, BATCH_SIZE, \n",
        "         rnn_units, embedding_dim,\n",
        "         loss.name, model, history,\n",
        "         pred_char_len, temp, result, \n",
        "         one_step_model]\n",
        "print('Model 9 is our 2nd LSTM model using the class NewModel2 \\n',\n",
        "      'Ive tried to mess around with the model so that it passes the state\\n',\n",
        "      'From the persective of training loss, it performs _____ than baseline (Model 2)',np.round(save9[1],2), 'versus', np.round(save2[1],2), '\\n',\n",
        "      'The predicted text is totally incoherent :\\n', 'Model 9 :', save9[13].numpy()[0], '\\n',\n",
        "      'I have definitely made an error in how I set up this LSTM network \\n ')\n",
        "print(save9, '\\n\\n')\n",
        "\n"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bleu_score', 'Training loss (final)', 'Training sequence length', 'Training Buffer Size', 'Training Epochs', 'Training Batch Size', 'RNN model: RNN units', 'RNN model: Embedding Dim', 'RNN model: Name of loss function', 'RNN model: Summary', 'RNN model: history object', 'Prediction: character length', 'Prediction: temp constant', 'Prediction: result', 'Prediction: model object'] \n",
            "\n",
            "Model 1 is the `custom` model using the customized model object in the original code\n",
            " There is no calculation of bleu score for this one, because I havent figured out at this point\n",
            " how to best evaluate the model. \n",
            "\n",
            "['NA', 1.1910383701324463, 100, 10000, 10, 64, 1024, 256, 'sparse_categorical_crossentropy', <__main__.CustomTraining object at 0x7efee66c6cd0>, <tensorflow.python.keras.callbacks.History object at 0x7eff4b279410>, 50, 1.0, <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
            "array([b'ROMEO:\\nO, thy sun doth let\\nRend her he should he knife.\\n'],\n",
            "      dtype=object)>, <__main__.OneStep object at 0x7efea8986a50>] \n",
            "\n",
            "\n",
            "Model 2 is our baseline model using the class MyModel with architecture the same as in the original code\n",
            " From the persective of training loss, it performs nearly as well as the previous custom model\n",
            " 1.2 versus 1.19 \n",
            "\n",
            "['NA', 1.2036826610565186, 100, 10000, 10, 64, 1024, 256, 'sparse_categorical_crossentropy', <__main__.MyModel object at 0x7efee66da950>, <tensorflow.python.keras.callbacks.History object at 0x7efee63f06d0>, 50, 1.0, <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
            "array([b'ROMEO:\\nUntim life is he not to guess.\\n\\nMARCIUS:\\nIs there'],\n",
            "      dtype=object)>, <__main__.OneStep object at 0x7efea7e72810>] \n",
            "\n",
            "\n",
            "Model 3 is our baseline model using the class MyModel with architecture the same as in the original code\n",
            " to highlight the impact of epochs on training performance, we have reduced it from 10 to 2 \n",
            " From the persective of training loss, it performs worse than before 2.0 versus 1.2 \n",
            " The predicted text is less coherent, too:\n",
            " Model 3 : b'ROMEO:\\nThe wontely he soum, as\\nI sape!\\nAnd swant wean I ' \n",
            " Model 2 : b'ROMEO:\\nUntim life is he not to guess.\\n\\nMARCIUS:\\nIs there' \n",
            "\n",
            "['NA', 2.0047507286071777, 100, 10000, 2, 64, 1024, 256, 'sparse_categorical_crossentropy', <__main__.MyModel object at 0x7efea7c7bf10>, <tensorflow.python.keras.callbacks.History object at 0x7efea5fa34d0>, 50, 1.0, <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
            "array([b'ROMEO:\\nThe wontely he soum, as\\nI sape!\\nAnd swant wean I '],\n",
            "      dtype=object)>, <__main__.OneStep object at 0x7efea7986b10>] \n",
            "\n",
            "\n",
            "Model 4 is our baseline model using the class MyModel with architecture the same as in the original code\n",
            " to highlight the impact of epochs on training performance, we have increased it from 2 to 30 \n",
            " From the persective of training loss, it performs better than before 0.48 versus 2.0 \n",
            " The predicted text is way more coherent, almost Shakespearean:\n",
            " Model 4 : b'ROMEO:\\nThou art well where you will hear none.\\n\\nGREMIO:\\n' \n",
            " Model 3 : b'ROMEO:\\nThe wontely he soum, as\\nI sape!\\nAnd swant wean I ' \n",
            " This performance bump does come at the expense of time, though (20s / epoch * 30 epochs = 10 minutes, \n",
            "\n",
            "['NA', 0.48344069719314575, 100, 10000, 30, 64, 1024, 256, 'sparse_categorical_crossentropy', <__main__.MyModel object at 0x7efea7090f50>, <tensorflow.python.keras.callbacks.History object at 0x7efea5fa3250>, 50, 1.0, <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
            "array([b'ROMEO:\\nThou art well where you will hear none.\\n\\nGREMIO:\\n'],\n",
            "      dtype=object)>, <__main__.OneStep object at 0x7efea64a4e90>] \n",
            "\n",
            "\n",
            "Model 5 is our baseline model using the class MyModel with architecture the same as in the original code\n",
            " to explore ways to speed up training and preserve text coherence, I reduce the size of the input text \n",
            " by two orders of magnitude: 1115394 characters to 11153 characters and keep epochs = 30 \n",
            " From the persective of training loss, it performs far worse than before 3.12 versus 0.48 \n",
            " The predicted text is nonsense :\n",
            " Model 6 : b\"ROMEO:hEOl',hbta:zNji t atut sAk :nalrt rey e urn m la t\" \n",
            " Model 5 : b'ROMEO:\\nThou art well where you will hear none.\\n\\nGREMIO:\\n' \n",
            " This might be because of undertraining, or an issue in how weve indexed the vocabulary \n",
            " This saves training speed substantially! - 1 s / epoch \n",
            "\n",
            "['NA', 3.115530014038086, 100, 10000, 30, 64, 1024, 256, 'sparse_categorical_crossentropy', <__main__.MyModel object at 0x7efea63b7c10>, <tensorflow.python.keras.callbacks.History object at 0x7efea6362390>, 50, 1.0, <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
            "array([b\"ROMEO:hEOl',hbta:zNji t atut sAk :nalrt rey e urn m la t\"],\n",
            "      dtype=object)>, <__main__.OneStep object at 0x7efea60b6850>] \n",
            "\n",
            "\n",
            "Model 6 is our baseline model using the class MyModel, returning to epochs =  10 and characters = 1115394\n",
            " Here we start to vary the architecture of the model, so rnn_units = 2048 up from 1024 \n",
            " From the persective of training loss, it performs better than baseline (Model 2) 0.97 versus 1.2 \n",
            " The predicted text is probably more coherent :\n",
            " Model 6 : b'ROMEO:\\nThou art not half; he hath not shown but health\\na' \n",
            " Model 2 : b'ROMEO:\\nUntim life is he not to guess.\\n\\nMARCIUS:\\nIs there' \n",
            " Worth noting that the run-time nearly tripled, from 20 s / epoch to 55 s / epoch \n",
            "\n",
            "['NA', 0.9703238010406494, 100, 10000, 10, 64, 2048, 256, 'sparse_categorical_crossentropy', <__main__.MyModel object at 0x7efea4bf0c50>, <tensorflow.python.keras.callbacks.History object at 0x7efea4c0a310>, 50, 1.0, <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
            "array([b'ROMEO:\\nThou art not half; he hath not shown but health\\na'],\n",
            "      dtype=object)>, <__main__.OneStep object at 0x7efea6d12150>] \n",
            "\n",
            "\n",
            "Model 7 is our baseline model using the class MyModel \n",
            " Where we decrease the rnn_units = 256 down from 2048 \n",
            " From the persective of training loss, it performs worse than baseline (Model 2) 1.43 versus 1.2 \n",
            " The predicted text is arguably no more or less coherent :\n",
            " Model 7 : b\"ROMEO:\\nO! therefaleness of 'I thremfort\\nWhere is within \" \n",
            " Model 2 : b'ROMEO:\\nUntim life is he not to guess.\\n\\nMARCIUS:\\nIs there' \n",
            " Worth noting that the run-time didnt change very much \n",
            " \n",
            "['NA', 1.4263566732406616, 100, 10000, 10, 64, 256, 256, 'sparse_categorical_crossentropy', <__main__.MyModel object at 0x7efe42a21290>, <tensorflow.python.keras.callbacks.History object at 0x7efe42bf8c90>, 50, 1.0, <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
            "array([b\"ROMEO:\\nO! therefaleness of 'I thremfort\\nWhere is within \"],\n",
            "      dtype=object)>, <__main__.OneStep object at 0x7efef3be2b10>] \n",
            "\n",
            "\n",
            "Model 8 is our first LSTM model using the class NewModel1 \n",
            " Where we decrease the rnn_units = 1024 \n",
            " From the persective of training loss, it performs _____ than baseline (Model 2) 1.31 versus 1.2 \n",
            " The predicted text is totally incoherent :\n",
            " Model 8 : b'ROMEO:\\nAg:\\nAnge notifaloulyo ced bleathird thostll?\\nThit' \n",
            " I have definitely made an error in how I set up this recurrent network \n",
            " \n",
            "['NA', 1.313532829284668, 100, 10000, 10, 64, 1024, 256, 'sparse_categorical_crossentropy', <__main__.NewModel1 object at 0x7efef3f3a090>, <tensorflow.python.keras.callbacks.History object at 0x7efef3ffc090>, 50, 1.0, <tf.Tensor: shape=(1,), dtype=string, numpy=\n",
            "array([b'ROMEO:\\nAg:\\nAnge notifaloulyo ced bleathird thostll?\\nThit'],\n",
            "      dtype=object)>, <__main__.OneStep object at 0x7efe42a25810>] \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ11CEVL7t89"
      },
      "source": [],
      "execution_count": 278,
      "outputs": []
    }
  ]
}